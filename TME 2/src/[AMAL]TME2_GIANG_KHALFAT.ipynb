{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ec082e5f",
   "metadata": {},
   "source": [
    "### Etudiant 1: GIANG Phuong-Thu, Cécile (3530406)\n",
    "### Etudiant 1: KHALFAT Célina (28716860)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cd790312",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importation du code source\n",
    "import tp2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecc07b26",
   "metadata": {},
   "source": [
    "## 1 - Différenciation automatique : autograd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5f0c589",
   "metadata": {},
   "source": [
    "Nous implémentons tout d'abord une descente de gradient pour la régression linéaire. Contrairement au TME précédent, nous n'utilisons plus de contexte mais les fonctionnalités de la différenciation automatique proposée par `PyTorch`.\n",
    "\n",
    "Nous testons notre descente de gradient pour différentes stratégies pour la sélection d'échantillons."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54db5c84",
   "metadata": {},
   "source": [
    "**Descente de gradient batch:**\n",
    "\n",
    "<table>\n",
    "  <tr>\n",
    "    <td><img src=\"images/linear_reg_batch_train.png\" width=\"300\" height=\"480\"/></td>\n",
    "    <td><img src=\"images/linear_reg_batch_test.png\" width=\"300\" height=\"480\"/></td>\n",
    "  </tr>\n",
    " </table>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f3742e2",
   "metadata": {},
   "source": [
    "**Descente de gradient stochastique:**\n",
    "\n",
    "<table>\n",
    "  <tr>\n",
    "    <td><img src=\"images/linear_reg_stochastic_train.png\" width=300 height=480></td>\n",
    "    <td><img src=\"images/linear_reg_stochastic_test.png\" width=300 height=480></td>\n",
    "  </tr>\n",
    " </table>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cc74fd2",
   "metadata": {},
   "source": [
    "**Descente de gradient mini-batch:**\n",
    "\n",
    "<table>\n",
    "  <tr>\n",
    "    <td><img src=\"images/linear_reg_minibatch_train.png\" width=300 height=480></td>\n",
    "    <td><img src=\"images/linear_reg_minibatch_test.png\" width=300 height=480></td>\n",
    "  </tr>\n",
    " </table>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3db3bc4",
   "metadata": {},
   "source": [
    "La descente de gradient par batch est parmi nos trois stratégies la plus chronophage, puisque chaque mise-à-jour des paramètres s'effectue après une estimation de la loss sur l'ensemble des échantillons. La stochastique, quant à quelle, permet de trouver plus rapidement des paramètres améliorés mais la convergence est très instable, notre apprentissage est donc très peu fiable.\n",
    "\n",
    "Au final, nous concluons que la descente mini-batch est dans notre cas la stratégie la plus adaptée, puisqu'elle combine vitesse et stabilité sur la convergence."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ea4e5bd",
   "metadata": {},
   "source": [
    "## 2 - Optimiseur"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "469cb88b",
   "metadata": {},
   "source": [
    "Nous ré-implémentons cette même dscente de gradient sur la régression linéaire, mais en utilisant cette fois-ci l'optimiseur de `PyTorch` qui a pour avantages:\n",
    "- d’économiser quelques lignes de codes\n",
    "- d’automatiser la mise-à-jour des paramètres\n",
    "- d’abstraire le type de descente de gradient utilisé (SGD, Adam, rmsprop, ...)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8f0fac1",
   "metadata": {},
   "source": [
    "**Descente de gradient batch:**\n",
    "\n",
    "<table>\n",
    "  <tr>\n",
    "    <td><img src=\"images/optim_linear_reg_batch_train.png\" width=300 height=480></td>\n",
    "    <td><img src=\"images/optim_linear_reg_batch_test.png\" width=300 height=480></td>\n",
    "  </tr>\n",
    " </table>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9d559f2",
   "metadata": {},
   "source": [
    "**Descente de gradient stochastique:**\n",
    "\n",
    "<table>\n",
    "  <tr>\n",
    "    <td><img src=\"images/optim_linear_reg_stochastic_train.png\" width=300 height=480></td>\n",
    "    <td><img src=\"images/optim_linear_reg_stochastic_test.png\" width=300 height=480></td>\n",
    "  </tr>\n",
    " </table>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85a66a96",
   "metadata": {},
   "source": [
    "**Descente de gradient mini-batch:**\n",
    "\n",
    "<table>\n",
    "  <tr>\n",
    "    <td><img src=\"images/optim_linear_reg_minibatch_train.png\" width=300 height=480></td>\n",
    "    <td><img src=\"images/optim_linear_reg_minibatch_test.png\" width=300 height=480></td>\n",
    "  </tr>\n",
    " </table>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26ba3903",
   "metadata": {},
   "source": [
    "Mêmes observations que précédemment: la stratégie mini-batch nous permet d'allier rapidité et stabilité de la convergence."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9668612",
   "metadata": {},
   "source": [
    "## 3 - Module"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b2a25df",
   "metadata": {},
   "source": [
    "Nous souhaitons maintenant utiliser les modules `torch.nn.Linear`, `torch.nn.Tanh` et `torch.nn.MSELoss` pour implémenter un réseau de neurones à deux couches selon le schéma suivant: `lineaire → tanh → lineaire → MSE`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06b79456",
   "metadata": {},
   "source": [
    "### Version modulaire générique"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0e74c5f",
   "metadata": {},
   "source": [
    "**Descente de gradient batch:**\n",
    "\n",
    "<table>\n",
    "  <tr>\n",
    "    <td><img src=\"images/nn_gen_batch_train.png\" width=300 height=480></td>\n",
    "    <td><img src=\"images/nn_gen_batch_test.png\" width=300 height=480></td>\n",
    "  </tr>\n",
    " </table>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86e35e8e",
   "metadata": {},
   "source": [
    "**Descente de gradient stochastique:**\n",
    "\n",
    "<table>\n",
    "  <tr>\n",
    "    <td><img src=\"images/nn_gen_stochastic_train.png\" width=300 height=480></td>\n",
    "    <td><img src=\"images/nn_gen_stochastic_test.png\" width=300 height=480></td>\n",
    "  </tr>\n",
    " </table>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caab5e68",
   "metadata": {},
   "source": [
    "**Descente de gradient mini-batch:**\n",
    "\n",
    "<table>\n",
    "  <tr>\n",
    "    <td><img src=\"images/nn_gen_minibatch_train.png\" width=300 height=480></td>\n",
    "    <td><img src=\"images/nn_gen_minibatch_test.png\" width=300 height=480></td>\n",
    "  </tr>\n",
    " </table>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "333e9c89",
   "metadata": {},
   "source": [
    "### Version séquentielle"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e387e9e0",
   "metadata": {},
   "source": [
    "**Descente de gradient batch:**\n",
    "\n",
    "<table>\n",
    "  <tr>\n",
    "    <td><img src=\"images/nn_seq_batch_train.png\" width=300 height=480></td>\n",
    "    <td><img src=\"images/nn_seq_batch_test.png\" width=300 height=480></td>\n",
    "  </tr>\n",
    " </table>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fb8da61",
   "metadata": {},
   "source": [
    "**Descente de gradient stochastique:**\n",
    "\n",
    "<table>\n",
    "  <tr>\n",
    "    <td><img src=\"images/nn_seq_stochastic_train.png\" width=300 height=480></td>\n",
    "    <td><img src=\"images/nn_seq_stochastic_test.png\" width=300 height=480></td>\n",
    "  </tr>\n",
    " </table>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b8ec9b8",
   "metadata": {},
   "source": [
    "**Descente de gradient mini-batch:**\n",
    "\n",
    "<table>\n",
    "  <tr>\n",
    "    <td><img src=\"images/nn_seq_minibatch_train.png\" width=300 height=480></td>\n",
    "    <td><img src=\"images/nn_seq_minibatch_test.png\" width=300 height=480></td>\n",
    "  </tr>\n",
    " </table>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf25c5fe",
   "metadata": {},
   "source": [
    "Pour chacune des trois stratégies, la loss est essentiellement la même quel que soit la modélisation adoptée (générique ou séquentielle). Le choix de modélisation repose seulement sur une préférence personnelle et ne résulte en aucun ajout de performance."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
